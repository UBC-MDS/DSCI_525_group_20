{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI 525: Milestone 1 Group 20\n",
    "\n",
    "## Group Members\n",
    "- Lauren Zung\n",
    "- Xinru Lu\n",
    "- Spencer Gerlach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 & 2: Contract & Repo\n",
    "\n",
    "- Completed by Lauren Zung"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Downloading the Data\n",
    "\n",
    "- Spencer Gerlach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change directory to location files stored.\n",
    "\n",
    "- Assuming we can't save data to our repo.\n",
    "\n",
    "> Will need to be updated depending on who is running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /Users/spencergerlach/Desktop/figshare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Complete metadata required for API request\n",
    "\n",
    "article_id = 14096681\n",
    "url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"figshare-nswrain\" # update depending on user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET request\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "files = data[\"files\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, download the file `data.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "files_to_dl = [\"data.zip\"]\n",
    "for file in files:\n",
    "    if file[\"name\"] in files_to_dl:\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + \"/\" + file[\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), \"r\") as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check all the file names\n",
    "%ls -ltr /Users/spencergerlach/Desktop/figshare/figshare-nswrain/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 4: Combine the Files with Python\n",
    "\n",
    "- Spencer Gerlach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"/Users/spencergerlach/Desktop/figshare/figshare-nswrain/AWI-ESM-1-1-LR_daily_rainfall_NSW.csv\")\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test2 = pd.read_csv(\"/Users/spencergerlach/Desktop/figshare/figshare-nswrain/ACCESS-CM2_daily_rainfall_NSW.csv\")\n",
    "df_test2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From these results, we can now proceed with reading and combining all CSVs (except `observed_daily_rainfall_SYD.csv`).\n",
    "\n",
    "- Use columns from the test CSVs above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Combine into one CSV\n",
    "files = glob.glob('/Users/spencergerlach/Desktop/figshare/figshare-nswrain/*.csv') \n",
    "# Manually removed observed_daily_rainfall_SYD.csv from the data folder\n",
    "df = pd.concat((pd.read_csv(file).assign(model=re.findall(\"/([^_]*)\", file)[0]) for file in files))\n",
    "df.to_csv(\"/Users/spencergerlach/Desktop/figshare/figshare-nswrain/combined_data.csv\") # Use absolute path for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Part 4: Time Taken to Combine CSV files\n",
    "\n",
    "| Team Member | Operating System | RAM | Processor | Is SSD | Time Taken |\n",
    "|-------------|------------------|-----|-----------|--------|------------|\n",
    "|  Spencer    |   MacOS 12.6     |  8  | intel i5  |   Yes  |  16m 5s    |\n",
    "|  Xinru      |   MacOS 13.2     | 16  | Apple M2  |   Yes  |  3m 50s    |\n",
    "|             |                  |     |           |        |            |\n",
    "|             |                  |     |           |        |            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 5: Load the combined CSV to memory and perform a simple EDA\n",
    "\n",
    "- Xinru Lu\n",
    "\n",
    "1. Changing dtype of the data\n",
    "2. Load just columns that we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# local path to combined data (to be updated per user)\n",
    "combined_data_path = 'data/figshare-nswrain/combined_data.csv'\n",
    "\n",
    "# define column dtypes and columns to load\n",
    "column_dtype = {'lat_min': np.float32, 'lat_max': np.float32, 'lon_min': np.float32, 'lon_max': np.float32, 'model': str}\n",
    "use_columns = ['time', 'lat_min', 'lat_max', 'lon_min', 'lon_max', 'rain (mm/day)', 'model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.read_csv(combined_data_path, dtype=column_dtype, parse_dates=['time'], usecols=use_columns)\n",
    "print(df[['lat_min', 'lat_max', 'lon_min', 'lon_max']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 5: Time Taken to Load CSV files\n",
    "\n",
    "| Team Member | Operating System | RAM | Processor | Is SSD | Time Taken |\n",
    "|-------------|------------------|-----|-----------|--------|------------|\n",
    "|  Spencer    |   MacOS 12.6     |  8  | intel i5  |   Yes  |            |\n",
    "|  Xinru      |   MacOS 13.2     | 16  | Apple M2  |   Yes  |  46.9 s    |\n",
    "|             |                  |     |           |        |            |\n",
    "|             |                  |     |           |        |            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 6: Perform a simple EDA in R\n",
    "\n",
    "- Xinru Lu\n",
    "\n",
    "I would use **Arrow exchange** since it helps with minimizing the time-consuming serialization/deserialization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%reset -f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pandas as pd\n",
    "import pyarrow \n",
    "from pyarrow import csv\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepathparquet = \"data/figshare-nswrain/combined_data.parquet\"\n",
    "filepathparquetr = \"data/figshare-nswrain/combined_data_r.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Converting the `pyarrow dataset` to a `pyarrow table`\n",
    "table = pa.Table.from_pandas(df)\n",
    "# Converting a `pyarrow table` to a `rarrow table`\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
