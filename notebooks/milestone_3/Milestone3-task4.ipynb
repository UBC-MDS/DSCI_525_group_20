{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 4 (Guided Exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of Milestone 3, Question 4, and is a guided exercise. I have included guidelines and helpful links (as comments) along with this notebook to guide you through the exercise. Daniel's tutorial will also be helpful, but even if you haven't completed it, you should be able to finish this exercise with the help of the links and guidelines provided here. \n",
    "\n",
    "For some of you, this may be the first time exploring a package independently using documentation, so it might take some time to get used to it. But with practice, you will get better at it. At work, you may come across many packages that you have never used before, and you will have to learn them on your own. So, this is a good practice to get used to it.\n",
    "\n",
    "In this exercise, you will use Spark's MLlib. The idea is to tune some hyperparameters of a Random Forest to find the optimum model. Once you know the optimum settings, you will train a Random Forest in sklearn (`Milestone3-task3.ipynb` part 2) and save it with joblib (`Milestone3-task3.ipynb` part 2) so that you can use it next week to deploy.\n",
    "\n",
    "Here, consider MLlib as another Python package that you are using, like scikit-learn. You will see many scikit-learn similar classes and methods available in MLlib for various ML-related tasks. You may also notice that some of them are not yet implemented in MLlib. What you write using the PySpark package will use the spark engine to run your code, and hence, all the benefits of distributed computing that we discussed in class.\n",
    "\n",
    "Note: Whenever you use Spark, make sure that you refer to the right documentation based on the version you are using. You can select the version of Spark from [here](https://spark.apache.org/docs/)  and go to the correct documentation. In our case, we are using Spark 3.3.1, and here is the link to the Spark documentation that you can refer to:\n",
    "\n",
    "- [MLlib Documentation](https://spark.apache.org/docs/3.3.1/ml-guide.html)\n",
    "- [MLlib API Reference](https://spark.apache.org/docs/3.3.1/api/python/reference/pyspark.ml.html)\n",
    "\n",
    "You may notice that there are RDD-based API and DataFrame-based (Main Guide) API available in the documentation. You want to focus on DataFrame-based API as no one uses RDD-based API these days. We will discuss the difference in class.\n",
    "\n",
    "Before you start this notebook, make sure that you setup your EMR notebooks, uploaded this notebook there, and the kernel you selected is PySpark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install packages\n",
    "\n",
    "You only want to install following packages for this exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T00:37:14.467511Z",
     "iopub.status.busy": "2023-04-18T00:37:14.467285Z",
     "iopub.status.idle": "2023-04-18T00:38:08.354909Z",
     "shell.execute_reply": "2023-04-18T00:38:08.354098Z",
     "shell.execute_reply.started": "2023-04-18T00:37:14.467488Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6522c308afc44b25a1ad4ada5717b580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>6</td><td>application_1681773940944_0007</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-21-5.us-west-2.compute.internal:20888/proxy/application_1681773940944_0007/\" class=\"emr-proxy-link\" emr-resource=\"j-3L7ET2TEK7WE7\n",
       "\" application-id=\"application_1681773940944_0007\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-21-5.us-west-2.compute.internal:8042/node/containerlogs/container_1681773940944_0007_01_000001/livy\" >Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "Collecting python-dateutil>=2.7.3\n",
      "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3; platform_machine != \"aarch64\" and platform_machine != \"arm64\" and python_version < \"3.10\" in /usr/local/lib64/python3.7/site-packages (from pandas) (1.20.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.3.5 python-dateutil-2.8.2\n",
      "\n",
      "Collecting s3fs\n",
      "  Downloading s3fs-2023.1.0-py3-none-any.whl (27 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.8.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (948 kB)\n",
      "Collecting aiobotocore~=2.4.2\n",
      "  Downloading aiobotocore-2.4.2-py3-none-any.whl (66 kB)\n",
      "Collecting fsspec==2023.1.0\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "Collecting typing-extensions>=3.7.4; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (148 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "Collecting asynctest==0.13.0; python_version < \"3.8\"\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting attrs>=17.3.0\n",
      "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<4.0,>=2.0\n",
      "  Downloading charset_normalizer-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (171 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.8.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
      "Collecting botocore<1.27.60,>=1.27.59\n",
      "  Downloading botocore-1.27.59-py3-none-any.whl (9.1 MB)\n",
      "Collecting aioitertools>=0.5.1\n",
      "  Downloading aioitertools-0.11.0-py3-none-any.whl (23 kB)\n",
      "Collecting wrapt>=1.10.10\n",
      "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
      "Collecting importlib-metadata; python_version < \"3.8\"\n",
      "  Downloading importlib_metadata-6.4.1-py3-none-any.whl (22 kB)\n",
      "Collecting idna>=2.0\n",
      "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.0.1)\n",
      "Collecting urllib3<1.27,>=1.25.4\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in ./tmp/1681778257796-0/lib/python3.7/site-packages (from botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (2.8.2)\n",
      "Collecting zipp>=0.5\n",
      "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.27.60,>=1.27.59->aiobotocore~=2.4.2->s3fs) (1.13.0)\n",
      "Installing collected packages: typing-extensions, frozenlist, aiosignal, async-timeout, multidict, asynctest, zipp, importlib-metadata, attrs, charset-normalizer, idna, yarl, aiohttp, urllib3, botocore, aioitertools, wrapt, aiobotocore, fsspec, s3fs\n",
      "Successfully installed aiobotocore-2.4.2 aiohttp-3.8.4 aioitertools-0.11.0 aiosignal-1.3.1 async-timeout-4.0.2 asynctest-0.13.0 attrs-23.1.0 botocore-1.27.59 charset-normalizer-3.1.0 frozenlist-1.3.3 fsspec-2023.1.0 idna-3.4 importlib-metadata-6.4.1 multidict-6.0.4 s3fs-2023.1.0 typing-extensions-4.5.0 urllib3-1.26.15 wrapt-1.15.0 yarl-1.8.2 zipp-3.15.0\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"s3fs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T00:38:08.356112Z",
     "iopub.status.busy": "2023-04-18T00:38:08.355902Z",
     "iopub.status.idle": "2023-04-18T00:38:09.101219Z",
     "shell.execute_reply": "2023-04-18T00:38:09.100569Z",
     "shell.execute_reply.started": "2023-04-18T00:38:08.356091Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f18c890dee54c2bbd73b01bdeda48f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, UnivariateFeatureSelector\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.regression import RandomForestRegressor as sparkRFR\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with; read 100 data points for development purpose. Once your code is ready you should try on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T00:38:33.165791Z",
     "iopub.status.busy": "2023-04-18T00:38:33.165490Z",
     "iopub.status.idle": "2023-04-18T00:38:35.419887Z",
     "shell.execute_reply": "2023-04-18T00:38:35.419257Z",
     "shell.execute_reply.started": "2023-04-18T00:38:33.165766Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "431ab54aa9a54dad8c426d88f02c8a70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Remember by default it looks for credentials in home directory. \n",
    "## Makes sure your updated credentials are in home directory\n",
    "## Where this notebook is running? Clue: It is running on our master node. So you want to ssh into master node and update credentials there.\n",
    "## or pass credentials explicitly and pass as storage_options=aws_credentials (not a good idea)\n",
    "# aws_credentials = {\"key\": \"\",\"secret\": \"\",\"token\":\"\"}\n",
    "# replace with s3 path to your data\n",
    "## here 100 data points for testing the code, \n",
    "# pandas_df = pd.read_csv(\"s3://mds-s3-20-lauren/output/ml_data_SYD.csv\", index_col=0, parse_dates=True).iloc[:100].dropna()\n",
    "pandas_df = pd.read_csv(\"s3://mds-s3-20-lauren/output/ml_data_SYD.csv\", index_col=0, parse_dates=True).dropna()\n",
    "feature_cols = list(pandas_df.drop(columns=\"Observed\").columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing dataset for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T00:38:35.421125Z",
     "iopub.status.busy": "2023-04-18T00:38:35.420913Z",
     "iopub.status.idle": "2023-04-18T00:38:46.712279Z",
     "shell.execute_reply": "2023-04-18T00:38:46.711559Z",
     "shell.execute_reply.started": "2023-04-18T00:38:35.421104Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40c56afd91d747778b11d4c9749b2b23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataframe and coerce features into a single column called \"Features\"\n",
    "# This is a requirement of MLlib\n",
    "# Here we are converting your pandas dataframe to a spark dataframe, \n",
    "# Here \"spark\" is a spark session I will discuss in class. \n",
    "# It is automatically created for you in this notebook.\n",
    "# read more  here https://blog.knoldus.com/spark-createdataframe-vs-todf/\n",
    "training = spark.createDataFrame(pandas_df)\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"Features\")\n",
    "training = assembler.transform(training).select(\"Features\", \"Observed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T00:38:46.713839Z",
     "iopub.status.busy": "2023-04-18T00:38:46.713613Z",
     "iopub.status.idle": "2023-04-18T00:38:46.954700Z",
     "shell.execute_reply": "2023-04-18T00:38:46.954077Z",
     "shell.execute_reply.started": "2023-04-18T00:38:46.713816Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bcb0cc4cc694004b68d37fbaf9be556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[Features: vector, Observed: double]"
     ]
    }
   ],
   "source": [
    "training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find best hyperparameter settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Official Documentation of MLlib, Random forest regression [here](https://spark.apache.org/docs/3.3.1/ml-classification-regression.html#random-forest-regression).\n",
    "\n",
    "Here we will be mainly using following classes and methods;\n",
    "\n",
    "- [RandomForestRegressor](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.regression.RandomForestRegressor.html)\n",
    "- [ParamGridBuilder](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.ParamGridBuilder.html)\n",
    "    - addGrid\n",
    "    - build\n",
    "- [CrossValidator](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.tuning.CrossValidator.html)\n",
    "    - fit\n",
    "\n",
    "Use these parameters for coming up with ideal parameters, you could try more parameters, but make sure you have enough power to do it. But you are required to try only following parameters. This will take around 15 min on entire dataset....\n",
    "\n",
    "    - Use numTrees as [10, 50,100]\n",
    "    - maxDepth as [5, 10]\n",
    "    - bootstrap as [False, True]\n",
    "    - In the CrossValidator use evaluator to be `RegressionEvaluator(labelCol=\"Observed\")`\n",
    "    \n",
    "***Additional reference:*** You can refer to [here](https://www.sparkitecture.io/machine-learning/regression/random-forest) and [here](https://www.silect.is/blog/random-forest-models-in-spark-ml/).\n",
    "\n",
    "Some additional reading [here](https://projector-video-pdf-converter.datacamp.com/14989/chapter4.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-18T00:38:46.955860Z",
     "iopub.status.busy": "2023-04-18T00:38:46.955630Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77f8067c19ae4d5e9292022f8baa2895",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98fb266c2dcc4d9d937c30cde9d0563d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##Once you finish testing the model on 100 data points, then load entire dataset and run , this could take ~15 min.\n",
    "## write code below.\n",
    "#\n",
    "# Construct RFR\n",
    "rf = sparkRFR(labelCol=\"Observed\", featuresCol=\"Features\")\n",
    "\n",
    "# Set up parameter grid\n",
    "rf_paramGrid = (ParamGridBuilder()\n",
    "               .addGrid(rf.numTrees, [10, 50,100])\n",
    "               .addGrid(rf.maxDepth, [5, 10])\n",
    "               .addGrid(rf.bootstrap, [False, True]).build())\n",
    "\n",
    "# Set-up evaluator\n",
    "rf_evaluator = RegressionEvaluator(labelCol=\"Observed\")\n",
    "\n",
    "# Set-up cross-validation pipeline\n",
    "cv = CrossValidator(estimator = rf,\n",
    "                    estimatorParamMaps = rf_paramGrid,\n",
    "                    evaluator = rf_evaluator,\n",
    "                    numFolds = 5)\n",
    "\n",
    "# Run cross-validation\n",
    "cvModel = cv.fit(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print run info\n",
    "# cvModel is a variable that stores the best model obtained after performing cross-validation (crossval.fit(training))\n",
    "print(\"\\nBest model\")\n",
    "print(\"==========\")\n",
    "print(f\"\\nCV Score: {min(cvModel.avgMetrics):.2f}\")\n",
    "print(f\"numTrees: {cvModel.bestModel.getNumTrees}\")\n",
    "print(f\"maxDepth: {cvModel.bestModel.getMaxDepth()}\")\n",
    "print(f\"bootstrap: {cvModel.bestModel.getBootstrap()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  },
  "vscode": {
   "interpreter": {
    "hash": "6e9e0baa62560f8a3b402c12d339bdad33c58a25305700ec7e7682c0b6251f68"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
